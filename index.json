[{"uri":"https://EroEroz.github.io/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://EroEroz.github.io/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://EroEroz.github.io/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://EroEroz.github.io/3-blogstranslated/3.4-blog4/","title":"Blog 4","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://EroEroz.github.io/3-blogstranslated/3.5-blog5/","title":"Blog 5","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://EroEroz.github.io/3-blogstranslated/3.6-blog6/","title":"Blog 6","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://EroEroz.github.io/5-workshop/5.7-security/5.7.1-s3-cloudfront/","title":"Configure S3 &amp; CloudFront","tags":[],"description":"","content":"1. Create S3 Bucket:\nCreate Bucket (Ex: minimarket-assets-prod) Block Public Access: On Manually upload images folder from code to this Bucket 2. Create CloudFront Distribution:\nOrigin type: Select Elastic Load Balancer Origin Domain: Select Load Balancer of Beanstalk Settings: Select Customize origin settings Protocol: HTTP Only Cache settings: Select Customize cache settings Viewer Protocol Policy: Redirect HTTP to HTTPS 3. Add S3 Origin (To retrieve images):\nGo to newly created Distribution Go to Origins tab \u0026gt; Create Origin Origin domain select the S3 created earlier (minimarket-assets-prod) Origin Access: Select Origin access control (OAC) \u0026gt; Create new OAC Bucket Policy: Copy policy provided by CloudFront and paste into S3 Bucket policy 4. Configure Behavior:\nReturn to CloudFront go to Behaviors tab Create Behavior with Path pattern: /images/ Point to Origin S3 Cache Policy: CachingOptimized "},{"uri":"https://EroEroz.github.io/5-workshop/5.6-cicd/5.6.1-codebuild/","title":"Create Build Project","tags":[],"description":"","content":" Access CodeBuild \u0026gt; Create project\nProject name: MiniMarket-Build\nSource: Select GitHub (Connect to Repo containing code)\nEnvironment:\nEnvironment image: Managed Image Operating system: Amazon Linux Runtime: Standard Image: 5.0 Service role: New service role Privileged: Enable (Required to run Docker build commands) Buildspec: Use a buildspec file\nClick Create build project\nAfter creation is complete, go to IAM Role of the newly created CodeBuild, grant additional permission AmazonEC2ContainerRegistryPowerUser so it can push images to ECR\n"},{"uri":"https://EroEroz.github.io/5-workshop/5.3-network/5.3.1-create-vpc/","title":"Create VPC &amp; Subnets","tags":[],"description":"","content":" Open Amazon VPC console (Note: Choose region suitable for needs, here the group uses Region ap-southeast-1) Select Create VPC Configuration: Name: MiniMarket-VPC IPv4 CIDR: 10.0.0.0/16 Create Subnets (Split 2 AZs to ensure High Availability): Public Subnets (2): 10.0.1.0/24 \u0026amp; 10.0.2.0/24 (Used for Load Balancer \u0026amp; NAT) Private Subnets (2): 10.0.3.0/24 \u0026amp; 10.0.4.0/24 (Used for App, DB, Redis) Click Create VPC and wait for state to change to Available is successful "},{"uri":"https://EroEroz.github.io/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"“AWS Cloud Day 2025” Report Event Purpose Update cloud technology trends in key industries (Finance, Banking). Understand modern data strategies to support Generative AI. Grasp application modernization models on AWS. Highlights Financial Services \u0026amp; Modernization Dissolving boundaries: The trend of bridging the gap between Business and IT in financial institutions. Technology is a business driver, not just support. Modernization: Banks and payment organizations are shifting to Cloud for agility and customer experience. Data Analytics \u0026amp; GenAI Data Strategy: Data is fuel for AI. A solid data strategy is required before applying Generative AI. Integration: Integrating Foundation Models into data analytics workflows to improve decision-making effectiveness. Migrate, Modernize, and Build on AWS Cloud-native Architectures: Shifting from Monolith to Microservices and Serverless. Workload Modernization: Strategies for migrating and optimizing legacy applications on modern AWS platforms. What I Learned Modernization Mindset: Understanding that \u0026ldquo;moving to cloud\u0026rdquo; isn\u0026rsquo;t just moving servers, but re-architecting to Cloud-native to leverage scalability. Importance of Data: Generative AI is only effective with clean and well-organized data. Application to Work Apply Cloud-native thinking to the MiniMarket project (using Docker, Managed Services like RDS/ElastiCache instead of self-hosted on EC2). Consider data strategy for future scalable features (e.g., analyzing purchasing behavior). "},{"uri":"https://EroEroz.github.io/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Nguyen Hoang Gia Huy\nPhone Number: 0902566797\nEmail: huynhgse182631@fpt.edu.vn\nUniversity: FPT University\nMajor: Artificial Intelligence\nClass: AWS First Cloud Journey\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://EroEroz.github.io/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"Introduction MiniMarket is an e-commerce application built on the .NET Core platform, applying modern 3-Tier Architecture. The goal of this Workshop is to re-platform the application from an On-premise environment to AWS cloud infrastructure (Cloud Native Migration) ensuring AWS Well-Architected Framework criteria: Security, Reliability, Performance Efficiency, and Cost Optimization Workshop Overview Solution Architecture:\nCompute: Use AWS Elastic Beanstalk (Docker platform) to simplify deployment, infrastructure management, and Auto Scaling Database: Amazon RDS for SQL Server deployed in Private Subnet to ensure data security Caching: Amazon ElastiCache (Redis) helps store User Sessions and offload Database queries, increasing response speed Network \u0026amp; Security: VPC: Designed with Public/Private Subnet model combined with NAT Gateway Application Layer Security: Use AWS WAF combined with Amazon CloudFront to protect against Web attacks and distribute content globally Storage: Amazon S3 used to store and serve static assets (product images) with high durability DevOps: Fully automated CI/CD process with AWS CodePipeline and CodeBuild Monitoring: Amazon CloudWatch to monitor system health (CPU, Network) and send alerts "},{"uri":"https://EroEroz.github.io/5-workshop/5.8-monitoring/5.8.1-cloudwatch/","title":"Monitoring with CloudWatch","tags":[],"description":"","content":" Create SNS Topic:\nGo to SNS \u0026gt; Topics \u0026gt; Create Topic Type: Standard Name: DevOps-Alerts Create Subscription\nCreate Subscription \u0026gt; Protocol: Email \u0026gt; Enter your email (Remember to Confirm mail) Create CPU Alarm:\nGo to CloudWatch \u0026gt; Alarms \u0026gt; Create alarm Select metric \u0026gt; EC2 \u0026gt; Per-Instance Metrics \u0026gt; Select InstanceID of Beanstalk \u0026gt; CPUUtilization Condition: CPUUtilization: Greater than 70% Notification: Select Topic DevOps-Alerts Create Alarm. "},{"uri":"https://EroEroz.github.io/5-workshop/5.5-app/5.5.1-dockerize/","title":"Package with Docker","tags":[],"description":"","content":"Before moving to Cloud, we need to package the .NET Core application into a Docker Image\nCreate Dockerfile: At the root directory of the Solution, create a file named Dockerfile (no extension) ```dockerfile # STAGE 1: BUILD FROM mcr.microsoft.com/dotnet/sdk:8.0 AS build WORKDIR /src COPY [\u0026quot;MiniMarket.sln\u0026quot;, \u0026quot;./\u0026quot;] COPY [\u0026quot;WebShop/WebShop.csproj\u0026quot;, \u0026quot;WebShop/\u0026quot;] # ... (Copy other projects if any) RUN dotnet restore \u0026quot;MiniMarket.sln\u0026quot; COPY . . WORKDIR \u0026quot;/src/WebShop\u0026quot; RUN dotnet build \u0026quot;WebShop.csproj\u0026quot; -c Release -o /app/build RUN dotnet publish \u0026quot;WebShop.csproj\u0026quot; -c Release -o /app/publish # STAGE 2: RUNTIME FROM mcr.microsoft.com/dotnet/aspnet:8.0 AS final COPY --from=build /app/publish . WORKDIR /app EXPOSE 8080 ENTRYPOINT [\u0026quot;dotnet\u0026quot;, \u0026quot;WebShop.dll\u0026quot;] ENV ASPNETCORE_URLS=http://+:8080 ENV ASPNETCORE_ENVIRONMENT=Development ``` Create buildspec.yml: Create file buildspec.yml to instruct AWS CodeBuild how to package and push to ECR ```yaml version: 0.2 phases: pre_build: commands: - echo Logging in to Amazon ECR... # --- INFORMATION CONFIGURATION --- - AWS_DEFAULT_REGION=ap-southeast-1 # Replace your Account ID in the line below: - AWS_ACCOUNT_ID= YOUR ACCOUNT ID - IMAGE_REPO_NAME=market-app - IMAGE_TAG=$(echo $CODEBUILD_RESOLVED_SOURCE_VERSION | cut -c 1-7) - REPOSITORY_URI=$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME # --------------------------- - aws ecr get-login-password --region $AWS_DEFAULT_REGION | docker login --username AWS --password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com build: commands: - echo Build started on `date` - echo Building the Docker image... - docker build -t $REPOSITORY_URI:latest . - docker tag $REPOSITORY_URI:latest $REPOSITORY_URI:$IMAGE_TAG post_build: commands: - echo Build completed on `date` - echo Pushing the Docker image... - docker push $REPOSITORY_URI:latest - docker push $REPOSITORY_URI:$IMAGE_TAG - echo Writing image definitions file... # Automatically create Dockerrun.aws.json configuration file for Beanstalk # Map Port 80 (Host) to 8080 (Container .NET) - printf '{\u0026quot;AWSEBDockerrunVersion\u0026quot;:\u0026quot;1\u0026quot;,\u0026quot;Image\u0026quot;:{\u0026quot;Name\u0026quot;:\u0026quot;%s\u0026quot;,\u0026quot;Update\u0026quot;:\u0026quot;true\u0026quot;},\u0026quot;Ports\u0026quot;:[{\u0026quot;ContainerPort\u0026quot;:8080,\u0026quot;HostPort\u0026quot;:80}]}' \u0026quot;$REPOSITORY_URI:$IMAGE_TAG\u0026quot; \u0026gt; Dockerrun.aws.json - cat Dockerrun.aws.json artifacts: files: - Dockerrun.aws.json ``` "},{"uri":"https://EroEroz.github.io/5-workshop/5.9-cleanup/5.9.1-cleanup/","title":"Resource Cleanup","tags":[],"description":"","content":"To avoid unexpected costs after completing the Workshop, delete resources in the following correct order:\nNAT Gateway: Delete NAT Gateway \u0026gt; Wait for Deleted \u0026gt; Release Elastic IP (Most important as it costs the most) Elastic Beanstalk: Terminate Environment ElastiCache: Delete Redis Cluster (Uncheck Create Backup) RDS: Stop (or Delete if no longer in use - remember to uncheck Final Snapshot). WAF: Manage resources \u0026gt; Disassociate \u0026gt; Delete protection pack (web ACL) S3: Empty and Delete Bucket (Can skip deleting if still in use as cost is not too high)\nCloudFront Disable and Delete\n"},{"uri":"https://EroEroz.github.io/5-workshop/5.4-data/5.4.1-security-groups/","title":"Set up Security Groups","tags":[],"description":"","content":" Access EC2 \u0026gt; Security Groups \u0026gt; Create security group Group 1: Web Server (sg-web-app) Description: Allow HTTP from Internet Inbound Rules: Type: HTTP (80) | Source: 0.0.0.0/0 (Or only from Load Balancer if wanting stricter security) Group 2: Database (sg-db-sql) Description: Allow access only from Web Server Inbound Rules: Type: MSSQL (1433) | Source: Custom \u0026gt; Select ID of sg-web-app Group 3: Redis Cache (sg-redis-cache) Description: Allow access only from Web Server Inbound Rules: Type: Custom TCP (6379) | Source: Custom \u0026gt; Select ID of sg-web-app "},{"uri":"https://EroEroz.github.io/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Onboard with the First Cloud Journey (FCJ) community and understand rules. Set up AWS Free Tier account and development environment. Master basic AWS service categories (Compute, Storage, Networking, Database). Begin technical exploration of VPC and core services. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 3 - Onboarding \u0026amp; Account Setup + Met with FCJ members and reviewed community regulations + Created AWS Free Tier account + Explored AWS Management Console interface and finished Module 1 09/09/2025 09/09/2025 https://policies.fcjuni.com/ 4 - Environment \u0026amp; VPC Setup + Installed extensions to build the documentation site (Hugo/Worklog) + Started Module 2: Networking - Learned Virtual Private Cloud (VPC) concepts - Reviewed VPC Security features (Security Groups/NACLs) 09/10/2025 09/10/2025 https://cloudjourney.awsstudygroup.com/ 5 - Watched Module 2 Lab 3 to get an overview of the topic + Practiced networking concepts theory before implementation 09/11/2025 09/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Service Exploration (Hands-on) + Launched an EC2 Instance + Tested GenAI features in Amazon Bedrock Playground + Created a test function using AWS Lambda + Provisioned a test database using Amazon RDS 09/12/2025 09/12/2025 https://cloudjourney.awsstudygroup.com/ Week 1 Achievements: Successfully created and configured the AWS Account. Gained familiarity with the AWS Management Console and service navigation. Mastered the four basic service pillars: Compute, Storage, Networking, and Database. Learned fundamental cost optimization strategies for cloud projects. "},{"uri":"https://EroEroz.github.io/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Gain hands-on experience with Amazon EC2 creation and networking. Manage AWS account security and troubleshooting processes. Gain industry insights through AWS Cloud Day 2025. Begin the Natural Language Processing (NLP) specialization on Coursera. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - EC2 Lab \u0026amp; Troubleshooting + Public EC2 instance: Successfully created and verified connection + Private EC2 instance: Failed creation; account suspended during process + Created support ticket with AWS and terminated existing instances to save costs 09/15/2025 09/15/2025 https://000003.awsstudygroup.com/4-createec2server/4.2-connectec2/ https://www.youtube.com/watch?v=wWu67GyrUNY\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=42 3 - Watched AWS Module 2 lab videos while waiting for AWS support response - Enrolled in Coursera course: \u0026ldquo;Natural Language Processing with Classification and Vector Spaces\u0026rdquo; 09/16/2025 09/16/2025 https://www.youtube.com/watch?v=Oo2UpjL-exE\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=46 https://www.coursera.org/learn/classification-vector-spaces-in-nlp/home/module/1 4 - Checked AWS support status (Pending response) - Continued reviewing AWS lab materials - Progressed through Week 1 of the NLP course 09/17/2025 09/17/2025 https://www.youtube.com/watch?v=Oo2UpjL-exE\u0026list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i\u0026index=46 https://www.coursera.org/learn/classification-vector-spaces-in-nlp/home/module/1 5 - Attended Cloud Day 2025: + Financial Services: How AWS helps financial insitutions; dissolving boundaries between business/IT; modernization in payments, banking etc. + Data Analytics: Data strategy + generative AI, integrating foundation models, using data to improve AI effectiveness. + Migrate, Modernize, and Build on AWS: Cloud-native architectures, serverless/microservices, modernization of workloads. 09/18/2025 09/18/2025 6 - Finished Week 1 of Coursera NLP course + Completed programming assignment: Logistic Regression for Tweet Sentiment Analysis 09/19/2025 09/19/2025 https://www.coursera.org/learn/classification-vector-spaces-in-nlp/programming/P4CTb/logistic-regression Week 2 Achievements: Successfully launched and tested public EC2 instances. Initiated formal troubleshooting process for AWS account suspension. Gained strategic insights into FinTech, GenAI, and Cloud Migration via Cloud Day 2025. Completed Week 1 of the NLP course, applying logistic regression to real-world sentiment analysis data. "},{"uri":"https://EroEroz.github.io/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Complete Weeks 2 and 3 of the Coursera course \u0026ldquo;Natural Language Processing with Classification and Vector Spaces\u0026rdquo;. Resolve AWS Account status (Reinstatement) to prepare for cloud labs. Establish the Documentation Website structure for the semester. Formally establish team roles and development workflows. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Started Week 2 of Coursera NLP course - Followed up on AWS Support instructions for account reinstatement - Reviewed lab video guides for the upcoming modules 09/22/2025 09/22/2025 https://www.coursera.org/learn/classification-vector-spaces-in-nlp/home/module/2 3 - Team Meeting: Preliminary discussion on team structure - Completed Week 2 of NLP course + Finished Module 2 Lab 10 and the final assignment - Practiced LeetCode challenges 09/23/2025 09/23/2025 https://www.coursera.org/learn/classification-vector-spaces-in-nlp/programming/xDch8/naive-bayes 4 - Documentation Setup + Researched and customized the website structure (Hugo/GitHub Pages) for Worklogs and Proposal 09/24/2025 09/24/2025 5 - Started Week 3 of Coursera NLP course - Practiced 2 LeetCode problems 09/25/2025 09/25/2025 https://www.coursera.org/learn/classification-vector-spaces-in-nlp/home/module/3 6 - Completed Week 3 of Coursera NLP course - Team Meeting: Assigned initial roles (DevOps, Backend, Frontend) and agreed on communication tools 09/26/2025 09/26/2025 https://www.coursera.org/learn/classification-vector-spaces-in-nlp/programming/vbHXo/assignment-vector-space-models Week 3 Achievements: Accelerated progress by completing both Week 2 and Week 3 of the NLP Course. Successfully customized the Documentation Website to host project artifacts. Conducted critical team meetings to establish roles and communication workflows. Resolved AWS administrative issues and maintained coding practice. "},{"uri":"https://EroEroz.github.io/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Complete the final week of the Coursera course \u0026ldquo;Natural Language Processing with Classification and Vector Spaces\u0026rdquo;. Facilitate group brainstorming sessions to decide on the final project topic. Set up the local development environment (SQL Server, .NET SDK) for the upcoming project. Translate technical documentation regarding AWS Disaster Recovery. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Completed lectures for Week 4 of the NLP (Classification and Vector Spaces) course - Practiced 2 LeetCode problems to maintain coding agility 09/29/2025 09/29/2025 https://www.coursera.org/learn/classification-vector-spaces-in-nlp/home/module/4 3 - Group Project Brainstorming + Met with team to discuss potential project ideas (e-commerce vs. management systems) + Analyzed pros and cons of different tech stacks - Completed the final programming assignment for the NLP course 09/30/2025 09/30/2025 https://www.coursera.org/learn/classification-vector-spaces-in-nlp/programming/qFU3R/word-translation 4 - Technical Stack Selection + Confirmed decision to use .NET Core for backend and SQL Server for database - Continued LeetCode practice 10/01/2025 10/01/2025 5 - Database \u0026amp; Environment Setup + Installed SQL Server and SSMS locally + Configured local connection strings and verified connectivity + Researched database schema best practices for the potential e-commerce topic 10/02/2025 10/02/2025 https://learn.microsoft.com/en-us/sql/sql-server/ 6 - Translated blog \u0026ldquo;Cross-Region disaster recovery using AWS Elastic Disaster Recovery\u0026rdquo; - Reviewed and finalized Week 4 worklog entries 10/03/2025 10/03/2025 https://aws.amazon.com/blogs/storage/cross-region-disaster-recovery-using-aws-elastic-disaster-recovery/ Week 4 Achievements: Successfully completed the Coursera course \u0026ldquo;Natural Language Processing with Classification and Vector Spaces\u0026rdquo;. Finalized the Project Topic and Tech Stack (.NET/SQL) after team brainstorming. Established a working Local Development Environment ready for coding. Expanded cloud knowledge by translating technical blogs on AWS Disaster Recovery. "},{"uri":"https://EroEroz.github.io/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Start and progress through the Coursera course \u0026ldquo;Natural Language Processing with Probabilistic Models\u0026rdquo;. Continue AWS technical blog translation to enhance cloud knowledge. Maintain coding consistency by solving LeetCode problems. Draft initial sections of the Group Project Proposal (Problem Statement \u0026amp; Architecture). Ensure all translations follow proper format and style guidelines. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Translated the blog \u0026ldquo;Introducing the Amazon Braket Learning Plan\u0026rdquo; + Revised previous translated blog to match the correct format and style 10/06/2025 10/06/2025 https://aws.amazon.com/blogs/quantum-computing/introducing-the-amazon-braket-learning-plan-and-digital-badge/ 3 - Completed translation of the blog \u0026ldquo;Understanding and Remediating Cold Starts: An AWS Lambda Perspective\u0026rdquo; - Enrolled in the Coursera course \u0026ldquo;Natural Language Processing with Probabilistic Models\u0026rdquo; - Solved 1 LeetCode challenge for coding practice 10/07/2025 10/07/2025 https://aws.amazon.com/blogs/compute/understanding-and-remediating-cold-starts-an-aws-lambda-perspective/ 4 - Completed Week 1 of the Coursera course (NLP): + Finished lectures, quizzes, and assignments for the introductory module 10/08/2025 10/08/2025 https://www.coursera.org/learn/probabilistic-models-in-nlp 5 - Researched AWS Services for the project (Lambda, S3, API Gateway, DynamoDB) - Estimated operating costs using AWS Pricing Calculator - Drafted initial proposal sections: Executive Summary and Problem Statement 10/09/2025 10/09/2025 https://calculator.aws/#/ 6 - Designed the initial System Architecture Diagram for the team project - Wrote detailed documentation to help teammates visualize the workflow and component interactions - Started Week 2 of the Coursera course “Natural Language Processing with Probabilistic Models” 10/10/2025 10/10/2025 https://www.coursera.org/learn/probabilistic-models-in-nlp Week 5 Achievements: Completed Week 1 of the NLP Coursera course and started Week 2. Researched suitable AWS services and analyzed estimated operational costs. Drafted initial sections of the project proposal: Executive Summary and Problem Statement. Designed the initial System Architecture and documentation. Completed translations for AWS technical blogs regarding Quantum Computing and Lambda Cold Starts. "},{"uri":"https://EroEroz.github.io/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Complete Week 2 and Week 3 of the Coursera course “Natural Language Processing with Probabilistic Models”. Review and summarize key concepts from previous AWS labs related to serverless architecture. Continue developing the project proposal, focusing on technical implementation and architecture explanation. Collaborate with team members to refine system architecture and ensure service compatibility. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Continued Week 2 of Coursera course “Natural Language Processing with Probabilistic Models” - Revisited previous AWS labs to reinforce understanding of core services and workflows 10/13/2025 10/13/2025 https://www.coursera.org/learn/probabilistic-models-in-nlp/home/module/2 3 - Focus on and complete Week 2 of the NLP Coursera course - Modified and published the worklog page to GitHub Pages 10/14/2025 10/14/2025 https://www.coursera.org/learn/probabilistic-models-in-nlp/programming/8POp8/part-of-speech-tagging 4 - Started Week 3 of the Coursera course “Natural Language Processing with Probabilistic Models” 10/15/2025 10/15/2025 https://www.coursera.org/learn/probabilistic-models-in-nlp/home/module/3 5 - Attended the \u0026ldquo;Data Science On AWS\u0026rdquo; workshop: + Explored services like Amazon Textract and Amazon Polly + Learned about data preparation with SageMaker Canvas and Processing + Watched a tutorial on model building in SageMaker Studio - Reviewed material for upcoming AWS midterm exam - Attended web seminar \u0026ldquo;Reinventing DevSecOps with AWS Generative AI\u0026rdquo;: + Learned about the DevSecOps role and AI\u0026rsquo;s impact + Saw a demo of Amazon Q for code review and security issue detection 10/16/2025 10/16/2025 https://qhdn-hcmuni.fpt.edu.vn/2025/10/13/workshop-data-science-on-aws-mo-khoa-suc-manh-du-lieu-cung-dien-toan-dam-may/ https://www.facebook.com/share/v/1BnNV19jPs/ 6 - Continued progress on Week 3 of the Coursera course - Reviewed material for the upcoming AWS midterm exam - Redesigned the Project Architecture diagram 10/17/2025 10/17/2025 https://www.coursera.org/learn/probabilistic-models-in-nlp/home/module/3 Week 6 Achievements: Completed Week 2 of the Coursera course “Natural Language Processing with Probabilistic Models” Made significant progress on Week 3 of the Coursera course Expanded knowdledge by attending two events: \u0026ldquo;Data Science On AWS\u0026rdquo; workshop and \u0026ldquo;Reinventing DevSecOps with AWS Generative AI\u0026rdquo; Redesigned the project architecture to enhance system efficiency and service integration "},{"uri":"https://EroEroz.github.io/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Advance knowledge in Natural Language Processing (Probabilistic Models). Prepare for the upcoming AWS Midterm Exam. Refine project proposal components: Architecture and Cost Estimation. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Finished Week 3 of the Coursera course “Natural Language Processing with Probabilistic Models” - Reviewed the draft writing for the Project Proposal - Continued practicing for the AWS midterm exam 10/20/2025 10/20/2025 https://www.coursera.org/learn/probabilistic-models-in-nlp/programming/7GrBn/autocomplete 3 - Started Week 4 of the Coursera course (NLP) - Adjusted the System Architecture diagram for the project - Practiced mock questions for AWS midterm exam 10/21/2025 10/21/2025 https://www.coursera.org/learn/probabilistic-models-in-nlp/home/module/4 4 - Continued Week 4 of the Coursera course (NLP) - Performed focused review for AWS midterm exam topics 10/22/2025 10/22/2025 https://www.coursera.org/learn/probabilistic-models-in-nlp/home/module/4 5 - Made further progress on Week 4 of the NLP course - Researched AWS Service Pricing (EC2, RDS) to support architecture decisions - Reviewed study material for the upcoming exam 10/23/2025 10/23/2025 https://www.coursera.org/learn/probabilistic-models-in-nlp/home/module/4 6 - Consolidated notes for the AWS Midterm Exam - Finalized the weekly revisions for the Project Proposal 10/24/2025 10/24/2025 Week 7 Achievements: Completed Week 3 and progressed through Week 4 of the NLP Specialization. Refined the project\u0026rsquo;s System Architecture based on pricing research. Conducted initial research on AWS Pricing for budget planning. Maintained consistent study routine for the AWS Midterm. "},{"uri":"https://EroEroz.github.io/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Intensive review of core AWS services and architectural best practices. Identify and strengthen knowledge gaps through mock exams. Successfully complete the AWS Midterm Exam. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Focused on comprehensive review for the AWS midterm exam + Reviewed core services including EC2, S3, and IAM fundamentals 10/27/2025 10/27/2025 3 - Continued practicing with mock tests to identify weak areas + Deepened understanding of VPC Networking and Security Groups 10/28/2025 10/28/2025 4 - Performed an intensive review of exam topics + Focused on the Shared Responsibility Model and Cloud Economics 10/29/2025 10/29/2025 5 - Conducted a final review of all study materials + Consolidated notes and verified exam readiness 10/30/2025 10/30/2025 6 - Completed the AWS Midterm Exam 10/31/2025 10/31/2025 Week 8 Achievements: Completed multiple focused study sessions covering all core AWS modules. Strengthened technical knowledge in Networking, Compute, and Security. Successfully sat for the AWS Midterm Exam. "},{"uri":"https://EroEroz.github.io/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Complete the first comprehensive draft of the group project proposal. Define the Cloud Solution Architecture and technical roadmap. Conduct resource planning including AWS Budget Estimation and risk analysis. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Refined the Problem Statement to align with the technical solution scope 11/03/2025 11/03/2025 3 - Redrew the System Architecture diagram to include cloud components + Rewrote the Solution Architecture section detailing the AWS infrastructure 11/04/2025 11/04/2025 4 - Wrote the Technical Implementation strategy (Stack \u0026amp; Deployment) + Defined the Timeline \u0026amp; Milestones for the development phase 11/05/2025 11/05/2025 5 - Off (Health Check) 11/06/2025 11/06/2025 6 - Calculated Budget Estimation focused on AWS infrastructure costs + Analyzed technical risks and mitigation strategies for the Risk Assessment section 11/07/2025 11/07/2025 Week 9 Achievements: Successfully finished the first complete draft of the project proposal. Re-engineered the technical foundation by defining the Cloud Architecture. Authored key planning sections: Technical Implementation, Timeline, and Cloud Budgeting. Refined the Problem Statement to clarify the technical challenges being solved. "},{"uri":"https://EroEroz.github.io/1-worklog/","title":"Worklog","tags":[],"description":"","content":"Introduction\nThis worklog documents of my 12-week internship journey focusing on building cloud computing skills with AWS and exploring related technologies. Each week, I recorded my objectives, tasks, and achievements.\nWeek 1: Introduction to AWS Services and Cloud Environment Setup\nWeek 2: Account Troubleshooting, Cloud Day Insights, and NLP Kickoff\nWeek 3: Account Reinstatement, Accelerated NLP Progress, and Team Organization\nWeek 4: Project Ideation, Tech Stack Selection, and NLP Course Completion\nWeek 5: Cost Estimation, Initial Architecture Design, and NLP Continuation\nWeek 6: Architecture Redesign, AWS Workshops, and DevSecOps Insights\nWeek 7: AWS Exam Preparation, Pricing Research, and NLP Deep Dive\nWeek 8: Intensive Review and AWS Midterm Exam Completion\nWeek 9: Project Proposal Finalization and Solution Architecture Planning\nWeek 10: Advanced NLP Sprint (Sequence Models) and Proposal Completion\nWeek 11: Cloud Migration Preparation and Codebase Configuration\nWeek 12: AWS Cloud Deployment, CI/CD Pipeline, and Service Optimization\n"},{"uri":"https://EroEroz.github.io/5-workshop/5.3-network/5.3.2-gateways/","title":"Configure Internet &amp; NAT Gateway","tags":[],"description":"","content":"Create Internet Gateway In the VPC dashboard click on Internet gateways Then click on Create internet gateway In the Internet gateway creation section, name it as desired then click on Create internet gateway and wait for it to be created After the Internet gateway is finished creating go to Actions and click on Attach to VPC to attach it to the VPC created in the previous section Create NAT Gateway Create NAT Gateway placed in Public Subnet 1 Assign Elastic IP to have a static address to the Internet "},{"uri":"https://EroEroz.github.io/5-workshop/5.3-network/5.3.3-routing/","title":"Configure Route Table","tags":[],"description":"","content":"Create Route Table Click on Route tables section in VPC dashboard\nCreate 2 Route Tables, Public with Private\nPublic Route Table: For Public Route Table, in Routes section click Edit routes Point 0.0.0.0/0 to Internet Gateway And in Subnet associations section, assign to both Public Subnets Private Route Table: For Private Route Table, we will point 0.0.0.0/0 to NAT Gateway And in Subnet associations section, assign to both Private Subnets Separating Route Tables ensures that Databases in Private Subnet are never exposed directly to the Internet.\n"},{"uri":"https://EroEroz.github.io/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"“Data Science on AWS Workshop” Report Event Purpose Hands-on experience with AWS AI/ML services. Learn the data preparation and Machine Learning model building process with Low-code/No-code tools. Highlights AI Services Exploration Amazon Textract: Intelligent service for extracting text and data from scanned documents (OCR). Amazon Polly: Service that converts text into lifelike speech (Text-to-Speech). Data Prep \u0026amp; Modeling Amazon SageMaker Canvas: No-code tool helping Business Analysts build ML models visually. SageMaker Processing: Handling and cleaning data at scale. SageMaker Studio: Comprehensive Integrated Development Environment (IDE) for Machine Learning, from building, training to deploying models. What I Learned AI vs ML Distinction: Clearly understood the difference between using pre-trained AI services (like Polly, Textract) and building custom ML models on SageMaker. Low-code ML: Recognized the democratization of Machine Learning, enabling non-coders to generate value from data thanks to SageMaker Canvas. Application to Work Potential to integrate Amazon Polly into MiniMarket project to read product information for visually impaired users (Accessibility). Use Textract to automate input invoice data entry for the inventory management system. "},{"uri":"https://EroEroz.github.io/5-workshop/5.4-data/5.4.2-rds/","title":"Initialize Amazon RDS","tags":[],"description":"","content":" Access RDS Console \u0026gt; Subnet groups \u0026gt; Create DB subnet group Name: db-private-group Subnets: Select 2 AZs and select exactly 2 Private Subnets Go to Databases \u0026gt; Create database Engine options: Microsoft SQL Server (Express Edition) Templates: Free tier Settings: Set Master Password (remember for later use) Connectivity: VPC: VPC you created for Web Subnet group: db-private-group Public access: No VPC security group: Select Security group you created for database Click Create database "},{"uri":"https://EroEroz.github.io/5-workshop/5.5-app/5.5.2-beanstalk-setup/","title":"Initialize Elastic Beanstalk","tags":[],"description":"","content":"We will create an environment to run the application\nAccess Elastic Beanstalk \u0026gt; Create application App Name: MiniMarket-App Platform: Docker (Amazon Linux 2023) Application code: Select Sample application (To test infrastructure first) Network Configuration (Networking) - Extremely Important: VPC: Select the VPC you created for MiniMarket Instance settings: Public IP address: Uncheck Subnets: Select 2 Private Subnets EC2 security groups: Select sg-web-app Capacity: Environment type: Select Load balanced Load balancer network settings: Visibility: Public Subnets: Select 2 Public Subnets Click Create. The system will take about 5-7 minutes to initialize "},{"uri":"https://EroEroz.github.io/5-workshop/5.2-prerequiste/","title":"Prerequisites","tags":[],"description":"","content":"IAM permissions Attach AdministratorAccess in IAM permission policy to the AWS account for easier workflow Note: Using Administrator privileges is recommended only for the Workshop environment to ensure the deployment process is uninterrupted. In a real Production environment, adhere to the Least Privilege principle for each service { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Source Code GitHub repository containing .NET Core code and a valid Dockerfile "},{"uri":"https://EroEroz.github.io/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Digital Transformation for Mini-market on AWS Cloud Platform .NET 3-tier E-commerce Solution applying Repository and Unit of Work Pattern 1. Executive Summary This proposal presents an end-to-end solution for \u0026ldquo;Digital Transformation for Mini-market on AWS Cloud Platform\u0026rdquo;. Traditional mini-markets are currently facing three major challenges: (1) Manual inventory management (using Excel/notebooks) causing revenue loss and resource waste; (2) 100% dependence on offline sales channels, missing the growing e-commerce market and losing competitiveness; and (3) Slow operational processes (such as manual price lookup), resulting in poor customer experience.\nThe group\u0026rsquo;s solution is to build a comprehensive e-commerce and operations management platform. Regarding software architecture, the group will use .NET 3-tier architecture (ASP.NET Core MVC, EF Core) combined with Repository Pattern and Unit of Work Pattern. Regarding infrastructure architecture, it is designed according to the AWS Well-Architected Framework, running on AWS Elastic Beanstalk (for .NET backend), Amazon RDS for SQL Server (for database), and Amazon S3 (for static assets). The system is performance-optimized using CloudFront and ElastiCache, and secured using WAF, VPC, and NAT Gateway. The deployment process is fully automated using a CI/CD pipeline integrated with GitHub.\nBusiness benefits are immediate, including automated inventory management (reducing loss) and opening a new online revenue channel. Regarding investment, infrastructure costs in the first 12 months are nearly zero by leveraging AWS Free Tier (e.g., RDS Express Edition, EC2 t3.micro). Long-term operating costs (after Free Tier) are also very practical, estimated at around 138.06 USD/month for the entire system. With minimal initial investment and the ability to directly solve problems causing revenue loss, ROI is very high and almost instant.\nThe project is proposed to be implemented in 12 weeks, divided into 4 main phases: (1) Foundation \u0026amp; Architecture, (2) Core Feature Development, (3) AWS Integration \u0026amp; CI/CD, and (4) Finalization \u0026amp; Deployment. Expected results are measured by specific success metrics: reduce inventory errors by 90%, reduce checkout time by 50%, and achieve 20% revenue from online channels in the first 6 months. This solution not only solves immediate problems but also provides mini-markets with a scalable platform for data-driven decision-making in the future.\n2. Problem Statement Current Problem\nSmall and medium retail businesses, especially the traditional \u0026ldquo;mini-market\u0026rdquo; model in Vietnam, are operating based on outdated manual processes. In the context of an increasingly digitized market, failing to apply technology to the work environment has created several problems, directly affecting their survival and growth.\nKey Problems\nManual inventory management leads to inaccuracy in figures and resource waste: Most mini-markets currently manage thousands of product codes (SKUs) using notebooks or Excel files. Importing/exporting goods and end-of-day inventory checks rely entirely on manual counting and entry. This can lead to data errors because the manual entry process is prone to mistakes, typically product codes and quantities, causing significant discrepancies between \u0026ldquo;on book\u0026rdquo; data and \u0026ldquo;actual\u0026rdquo; stock. Checking goods manually like this also demands high resources as staff have to spend hours every day counting, reconciling, and correcting reports, instead of focusing on sales or customer service. Finally, financial loss occurs; when data is not entered accurately, store owners cannot control the condition of goods such as expired goods, damaged goods, or theft, leading to a loss of 5-10% of inventory value monthly. Dependence on offline sales, missing the E-commerce market: Typically, mini-markets in Vietnam mostly depend on walk-in customers (offline). They are also limited by geographical location (only serving the local area) and a familiar customer base. Stores like this stand outside the E-commerce market, missing the young customer base already accustomed to online shopping. They cannot compete on convenience such as 24/7 ordering or door-to-door delivery compared to large convenience store chains like Circle K or 7-Eleven and delivery apps like Grab and Shopee, leading to a risk of losing customers over time. Operational and customer experience issues: The payment and information lookup process in traditional mini-markets is usually very slow. When customers ask about price, product information, or promotions, staff (especially new staff) have to manually look up in notebooks, leading to wasted time. Making customers wait long for information lookup or checkout creates frustration and unprofessionalism. Staff waste too much time on simple, error-prone tasks (such as misreading prices due to bad handwriting), reducing the number of customers that can be served during peak hours. 3. Solution Architecture The architecture is designed to solve the stated problems by combining .NET 3-tier software architecture with AWS Managed Services. This architecture adheres to the principles of the AWS Well-Architected Framework, ensuring security, high performance, fault tolerance, and cost optimization.\nAWS Services Used\nAWS Elastic Beanstalk: PaaS (Platform as a Service) selected to deploy the .NET 3-tier application (including WebShop Presentation Layer and Application Services Layer). Beanstalk automates 100% of infrastructure management, including automatically creating Auto Scaling Group (ASG) to ensure scalability and cost savings.\nAmazon RDS (SQL Server): Database Service (Managed Relational Database Service) to host the Persistence Layer. SQL Server was chosen because the group\u0026rsquo;s .NET application was developed and optimized for SQL Server. Using RDS for SQL Server allows migrating the application to AWS without changing code in the Data Layer. RDS will also automate complex tasks such as daily backups, patching, and failover. regarding security, RDS is placed in a Private Subnet, inaccessible directly from the Internet, only allowing the application on Beanstalk to connect. And regarding cost optimization, to optimize costs in the initial phase, we can start with SQL Server Express Edition on RDS, this version is within the AWS Free Tier.\nAmazon S3: Object Storage Service. Used to store static assets such as product images, CSS files, and JavaScript. S3 provides extremely low cost and unlimited scalability.\nAmazon CloudFront: Content Delivery Network Service - CDN. CloudFront caches static files from S3 at servers (Edge Locations) globally, helping users load pages significantly faster. It helps reduce direct load on Beanstalk servers and helps the .NET application focus on processing logic.\nAmazon WAF and Route 53: WAF (Web Application Firewall) and Route 53 (DNS Service). WAF is associated with CloudFront to block common web attacks (such as SQL injection, XSS). Route 53 provides domain names for users.\nAmazon ElastiCache (Redis): In-memory data stores service. Helps maximize load reduction for RDS Database when there are repetitive queries (for example: retrieving the homepage product list). The .NET application will cache these \u0026ldquo;hot\u0026rdquo; data on ElastiCache, helping increase response speed. Similar to RDS, ElastiCache is also placed in a Private Subnet to ensure safety.\nNAT Gateway: Network Address Translation Service. NAT will provide secure Internet access for services in the Private Subnet (like Elastic Beanstalk). This allows servers to download security patches without being accessed directly from outside.\nAWS CodePipeline/CodeBuild: Continuous Integration and Deployment (CI/CD) services. These services are integrated with GitLab to automate the process: (1) CodeBuild builds .NET code, (2) CodePipeline deploys the new version to Elastic Beanstalk.\nData Flow\n[1]-[2] Users access the domain name (via Route 53) and are routed to CloudFront. Amazon WAF will filter this request.\n[3] (Static Flow) If it is a static file (image, css), CloudFront retrieves directly from Amazon S3.\n[4]-[6] (Dynamic Flow) If it is a dynamic request, CloudFront forwards via Internet Gateway to Application Load Balancer, then ALB sends the request to Elastic Beanstalk.\n[7]-[8] The .NET application (on Beanstalk) will check ElastiCache first, if not found, will query Amazon RDS.\n[9]-[10] When Elastic Beanstalk needs Internet access (to download patches), it will go through NAT Gateway then out to Internet Gateway.\n[11]-[14] (CI/CD Flow) When Dev pushes code to Github, CodePipeline and CodeBuild will automatically build and deploy the new version to Elastic Beanstalk.\n4. Technical Implementation Implementation Stages\nThe project will be divided into 4 main stages, lasting 12 weeks to ensure progress and quality:\nBuilding Technical Foundation: Focus on building the technical foundation, including finalizing the data model for main entities, setting up .NET 3-tier solution structure (Domain, Application, Persistence, WebShop), initializing repository on Github, and researching AWS services. (Weeks 1-4)\nBuilding Core Features: Complete Persistence Layer (Repositories, Unit of Work) and Application Layer (Services) for main tasks such as managing products, users, and orders. Simultaneously, WebShop Layer (Controllers, Views) will be built for login flows, shopping cart, payment, and start writing Unit Tests for Services. (Weeks 5-8)\nCloud Migration Preparation: Group refactors the source code for Cloud compatibility (migrating configurations to Environment Variables), drafts build scripts (buildspec.yml), and cleans up the project to prepare for the CI/CD process. (Weeks 9-11)\nFinalization and Deployment: Group provisions all resources on AWS (Elastic Beanstalk, RDS, ElastiCache, S3). Security and performance services (CloudFront, WAF) are configured, and the automated CI/CD pipeline is activated. Finally, UAT is performed, and system monitoring is established via CloudWatch. (Week 12)\nTechnical Requirements\nBackend: ASP.NET Core MVC 9.0. ORM: Entity Framework Core. Database: MS SQL Server 2022 (Local) and Amazon RDS for SQL Server (Cloud). Frontend: Bootstrap 5, jQuery, and Bootstrap Icons. Cloud Platform (AWS): Elastic Beanstalk, RDS, S3, CloudFront, WAF, Route 53, ElastiCache, VPC, NAT Gateway, CodePipeline, CodeBuild. Source Control: Git. Tools: Visual Studio 2022, Docker Desktop. Development Methodology\nApply Agile methodology (Scrum-like) to flexibly adjust according to requirements and ensure progress, adhering to the 4 proposed deployment stages. All work (features, bugs) will be tracked and managed via Kanban board, helping the group easily grasp the progress of each task (e.g., To Do, In Progress, Done). All new code must be reviewed via merge requests on Github before being merged into the main branch, ensuring consistent code quality.\nTesting Strategy\nTo ensure quality and stability, the group will perform 3 levels of testing. First is Unit Testing, focusing 100% on the Application Layer (e.g., ProductService, OrderService) by mocking repositories to isolate Business Logic, using standard .NET testing frameworks. The second level is Integration Testing, performed on the Staging environment (on Elastic Beanstalk) to check the interaction between the Application Layer and Persistence Layer (EF Core) with the real Amazon RDS Database. Finally, User Acceptance Testing will be performed on the Production environment for the group to check complete functional flows on the user interface such as \u0026ldquo;Register, Login, Payment\u0026rdquo;.\nDeployment Plan\nApply fully automated CI/CD process. The process is automatically triggered whenever Dev pushes code to Github. Github will send a webhook triggering AWS CodePipeline, this service will take the code and command AWS CodeBuild to compile the .NET project, run Unit Tests, and package the application into a .zip file. If CodeBuild succeeds, CodePipeline will take the .zip file and automatically deploy this new version to the Staging environment on Elastic Beanstalk.\n5. Roadmap \u0026amp; Milestones The project is planned to be executed in 12 weeks, divided into 4 main stages. This schedule ensures time for development, integration, and thorough testing.\nPhase 1 (Weeks 1 - 4): This stage focuses on building the technical foundation, including finalizing data models, setting up .NET 3-tier Solution Architecture, initializing Github Repository, and researching AWS services. The milestone of this stage is Solution Architecture and Repository established, along with AWS environment (VPC, Subnets).\nPhase 2 (Weeks 5 - 8): After Phase 1 is complete, the group will build core features, complete Persistence and Application Layers (Product Management, Orders) and basic feature flows on WebShop (Auth, Cart). The milestone is main feature flows (Login, View Product, Cart, Payment) operating stably locally, and Unit Tests for Services.\nPhase 3 (Weeks 9 - 11): This phase prepares for cloud migration. The group optimizes the source code (Refactoring), configures Environment Variables, drafts automation scripts (Buildspec), and cleans up the project for the CI/CD process.\nPhase 4 (Week 12): This final stage focuses on finalization and deployment, dependent on the stable Staging build from Phase 3. The group will configure security services (CloudFront, WAF, Route 53). The milestone is Version 1.0 successfully deployed to Production environment (Elastic Beanstalk), final User Acceptance Testing completed, and system monitored via CloudWatch.\n6. Budget Estimation Costs can be viewed on AWS Pricing Calculator\nOr download budget estimation file.\nInfrastructure Costs\nEstimates from AWS Pricing Calculator show the monthly operating cost of this architecture is 138.06 USD, with an upfront cost of 0.00 USD. The group\u0026rsquo;s cost optimization strategy focuses on maximizing AWS Free Tier usage and managed services. The figure of 138.06 USD/month is a realistic cost for long-term operation (after 12 months) of a complete, scalable, and highly secure e-commerce system.\nCosts for Elastic Beanstalk are broken down into the resources it manages: Amazon EC2 (1 instance t3a.small) costing 19.15 USD/month and Elastic Load Balancing (1 Application Load Balancer) costing 18.69 USD/month. Amazon VPC service costs 46.02 USD/month, this is the cost for 1 NAT Gateway, a mandatory component for Elastic Beanstalk\u0026rsquo;s Private Subnet security design. Regarding database and cache, Amazon RDS for SQL Server (Express Edition version on db.t3.micro) costs 25.86 USD/month, and Amazon ElastiCache (cache.t4g.micro) is 17.52 USD/month. Security and CI/CD services like WAF (7.20 USD/month), CloudFront (2.43 USD/month), Route 53 (0.90 USD/month), S3 (0.17 USD/month), and CodeBuild (0.12 USD/month) make up the remaining costs.\nThe most important cost optimization strategy is leveraging AWS Free Tier in the first 12 months. Although the total estimate is 138.06 USD/month, many core services herein (including EC2 t3a.small, RDS db.t3.micro, ElastiCache t4g.micro, S3, and CloudFront) are within the Free tier free for 12 months. Therefore, the actual operating cost in the first year will be significantly lower, mainly consisting of costs for NAT Gateway (46.02 USD) and WAF (7.20 USD).\nRegarding ROI calculation, and initial investment is nearly zero (as infrastructure costs are covered within Free Tier and development costs are the group\u0026rsquo;s effort during the internship). Profit is almost immediate, as the solution directly solves revenue loss problems (from manual inventory management) and missed market opportunities (due to offline-only sales) stated in the Problem Statement (Part 1). Therefore, ROI (return on investment) is very high.\n7. Risk Assessment Some potential risks lie in three areas: Technical, Business, and Operational. Thus a mitigation plan and contingency plan have been prepared for high-impact risks.\nTechnical: System Overload (Performance Bottleneck):\nImpact: High | Probability: Medium This is the risk of the system being slow or crashing when there is high traffic volume. The group\u0026rsquo;s mitigation strategy is to use ElastiCache to offload queries for RDS, configure Auto Scaling (in Elastic Beanstalk) with reasonable triggers (e.g., CPU \u0026gt; 70%), and use CloudFront to cache static files. The contingency plan is to use CloudWatch Alarms for immediate alerts, and if RDS overloads, the group will perform vertical scaling of the RDS instance immediately. Business: Low User Adoption:\nImpact: High | Probability: Medium This is the risk that mini-market owners find the solution too complex and do not use it. To mitigate this risk, the group will stick to a simple frontend design (Bootstrap), gather shop owner feedback early from Phase 2, and provide instruction manuals. The contingency plan is if adoption is low after deployment, the group will perform an additional Sprint (Phase 5) to prioritize adjusting features based on gathered feedback. Operational: Data Loss / Breach:\nImpact: Critical | Probability: Low The group\u0026rsquo;s mitigation strategy is to configure RDS for automatic daily backups, place RDS and Beanstalk in Private Subnet, use WAF to block attacks, and manage connection strings via Beanstalk environment variables. The contingency plan is if data is lost, the group will perform Point-in-Time Recovery (PITR) immediately from RDS backup. 8. Expected Outcomes The goal of this solution is to directly address the problems stated in the Problem Statement section. Regarding business metrics, the group expects to reduce inventory management errors by 90% (compared to Excel/notebooks), reduce checkout time at the counter by 50%, and achieve at least 20% new revenue from online channels in the first 6 months. Regarding technical metrics, the goal is to maintain 99.9% uptime, ensure average page load time under 2 seconds (thanks to CloudFront and ElastiCache), and stable deployment frequency via CI/CD pipeline.\nBenefits are expected to increase over time. In the short-term (0-6 months), mini-market owners will see immediately improved user experience and significantly improved operations (automated inventory management). In the medium-term (6-18 months), value comes from market expansion (reaching online customers) and starting to collect valuable business data. Long-term value and strategic capabilities gained are the ability to make data-driven decisions (e.g., knowing which products sell well) and easy system scalability (adding more new stores) thanks to Solution Architecture on Elastic Beanstalk and RDS.\n"},{"uri":"https://EroEroz.github.io/5-workshop/5.6-cicd/5.6.2-codepipeline/","title":"Set up CodePipeline","tags":[],"description":"","content":" Access CodePipeline \u0026gt; Create pipeline\nCategory: Select Build custom pipeline\nSettings: Select New service role\nSource Stage: Select GitHub (via GitHub App) \u0026gt; Connect to GitHub \u0026gt; Select Repo and branch that you deploy to cloud\nBuild Stage: Select AWS CodeBuild \u0026gt; Select project MiniMarket-Build just created\nTest Stage: Click Skip test stage\nDeploy Stage:\nProvider: AWS Elastic Beanstalk Application name: MiniMarket-App Environment name: Select running environment Click Create pipeline\nIf Deploy step has Permission error, go to IAM Role of CodePipeline and grant permission AdministratorAccess-AWSElasticBeanstalk\n"},{"uri":"https://EroEroz.github.io/5-workshop/5.7-security/5.7.2-waf/","title":"Set up Firewall (WAF)","tags":[],"description":"","content":" Access WAF \u0026amp; Shield \u0026gt; Protection packs (webs ACLs) \u0026gt; Create protection pack (web ACL) App category: E-commerce \u0026amp; transaction platforms App focus: Both API and web Add resources \u0026gt; Add CloudFront or Amplify resources \u0026gt; Select CloudFront distribution created in previous section Choose initial protections \u0026gt; Build your own pack from all of the protections AWS WAF offers \u0026gt; AWS-managed rule group:\nAdd Core rule set (Block bot, bad IP) Add SQL database (Block SQL Injection) Testing: Access URL: https://[domain]/?id=1 OR 1=1. If receiving error 403 Forbidden, WAF is active.\n"},{"uri":"https://EroEroz.github.io/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Finalize and complete the group project proposal by authoring the remaining \u0026ldquo;Expected Outcomes\u0026rdquo; and \u0026ldquo;Executive Summary\u0026rdquo; sections.\nConduct a final proofread of the proposal to ensure quality and correct any minor errors.\nBegin the next course in the NLP specialization, \u0026ldquo;Natural Language Processing with Sequence Models,\u0026rdquo; and establish a strong learning momentum.\nTasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Completed the project proposal: + Wrote the \u0026ldquo;Expected Outcomes\u0026rdquo; section + Wrote the \u0026ldquo;Executive Summary\u0026rdquo; section - Started Week 1 of the Coursera course \u0026ldquo;Natural Language Processing with Sequence Models\u0026rdquo; 11/10/2025 11/10/2025 https://www.coursera.org/learn/sequence-models-in-nlp/home/module/1 3 - Finished Week 1 of the Coursera course \u0026ldquo;Natural Language Processing with Sequence - Proofread and made minor adjustments to the project proposal 11/11/2025 11/11/2025 https://www.coursera.org/learn/sequence-models-in-nlp/home/module/1 4 - Started Week 2 of the Coursera course \u0026ldquo;Natural Language Processing with Sequence Models\u0026rdquo;: + Topic: LSTMs and Named Entity Recognition - Studied RNNs, Vanishing Gradients, LSTMs, and NER (Training, Data Processing, Accuracy) - Completed the Practice Assignment. 11/12/2025 11/12/2025 https://www.coursera.org/learn/sequence-models-in-nlp/home/module/2 5 - Continued Week 2 and Started Week 3 of the Coursera course \u0026ldquo;Natural Language Processing with Sequence Models\u0026rdquo;: + Finished Week 2: - Completed the programming assignment \u0026ldquo;Named Entity Recognition (NER)\u0026rdquo;. + Started Week 3: - Topic: Siamese Networks - Studied: - Siamese Networks architecture, cost function - Triplets (inc. hard triplets), computing cost - One-shot learning, and training/testing - Completed the Practice Assignment 11/13/2025 11/13/2025 https://www.coursera.org/learn/sequence-models-in-nlp/programming/hDP4K/named-entity-recognition-ner https://www.coursera.org/learn/sequence-models-in-nlp/home/module/3 6 - Finished the course \u0026ldquo;Natural Language Processing with Sequence Models\u0026rdquo;: + Completed the final programming assignment (Week 3): \u0026ldquo;Question\nDuplicates\u0026rdquo; - Started the course \u0026ldquo;Natural Language Processing with Attention Models\u0026rdquo;\n(Week 1): + Topic: Neural Machine Translation - Studied: - Seq2seq, Seq2seq with attention - Queries, Keys, Values, and Attention - Setup for Machine Translation, Teacher forcing - NMT Model with attention, and Bleu Score - Completed Lab: - Basic attention - Scaled Dot-Product Attention 11/14/2025 11/14/2025 https://www.coursera.org/learn/sequence-models-in-nlp/programming/jjKpq/question-duplicates https://www.coursera.org/learn/attention-models-in-nlp/home/module/1 Week 10 Achievements: Project Proposal Finalized: Successfully completed the entire project proposal, including the \u0026ldquo;Executive Summary\u0026rdquo; and \u0026ldquo;Expected Outcomes,\u0026rdquo; and performed a final proofread.\nCompleted \u0026ldquo;Sequence Models\u0026rdquo; Course: Demonstrated exceptional learning velocity by finishing the \u0026ldquo;Natural Language Processing with Sequence Models\u0026rdquo; course in one week.\nStarted \u0026ldquo;Attention Models\u0026rdquo; Course: Immediately began the next course, \u0026ldquo;Natural Language Processing with Attention Models,\u0026rdquo; and made significant progress, covering Seq2seq, Attention, QKV, and completing two labs (Basic Attention and Scaled Dot-Product Attention).\n"},{"uri":"https://EroEroz.github.io/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Audit and synchronize the latest backend codebase for deployment. Prepare application configuration (Environment Variables) for production. Draft build scripts and configuration files for AWS services. Finalize project structure for the upcoming cloud migration. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Attended \u0026ldquo;DevOps on AWS\u0026rdquo; full-day workshop + Morning Session (CI/CD \u0026amp; IaC): - Learned DevOps Mindset and metrics (DORA, MTTR) - Deep dived into AWS Code Suite (CodeCommit, CodeBuild, CodeDeploy, CodePipeline) - Explored Infrastructure as Code (IaC) using CloudFormation and CDK + Afternoon Session (Containers \u0026amp; Observability): - Covered Docker Fundamentals and Amazon ECR/ECS/EKS orchestration - Learned Monitoring \u0026amp; Observability best practices using CloudWatch and AWS X-Ray 11/17/2025 11/17/2025 3 - Codebase \u0026amp; Config Prep (Post-workshop implementation) + Synced with backend team to obtain the stable release branch + Sanitized appsettings.json and mapped Environment Variables for the cloud 11/18/2025 11/18/2025 4 - Drafted buildspec.yml commands for the upcoming CI/CD pipeline 11/19/2025 11/19/2025 5 - Removed unused files and debug logs from the project directory 11/20/2025 11/20/2025 6 - Reviewed the deployment checklist for next week + Confirmed with the team that no further code changes will be made before Monday 11/21/2025 11/21/2025 Week 11 Achievements: Successfully prepared the application configuration for a production environment. Cleaned up the project structure to ensure a smooth deployment. Confirmed project readiness for the Week 12 cloud migration. "},{"uri":"https://EroEroz.github.io/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Deploy and verify project stability on AWS Cloud. Implement CI/CD automation using AWS CodePipeline. Optimize static asset delivery using S3 and CloudFront. Implement distributed caching using Amazon ElastiCache (Redis). Begin AWS Cloud Architecture training. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Reviewed and merged latest group code changes to ensure consistency + Refactored local configuration files (appsettings) to prepare for AWS migration 11/24/2025 11/24/2025 3 - Test deploy project code on AWS Cloud to verify stability + Verify functions: add items to cart and payment + Configure services to ensure compatibility with group code - Enrolled in AWS Cloud Solutions Architect specialization on Coursera + Started Course 1: AWS Cloud Technical Essentials - Completed Week 1: AWS Overview and Security introductory materials - Covered What is AWS? and AWS Global Infrastructure (Video \u0026amp; Reading 1.3) 11/25/2025 11/25/2025 4 - Configure AWS CodePipeline for automated deployment + Set up CodeBuild and CodeDeploy services - Configure GitHub integration: code push triggers automatic deployment to AWS Cloud 11/26/2025 11/26/2025 5 - Initialize S3 Bucket and migrate static assets (images) + Configure CloudFront (CDN) to serve content from S3 + Implement security controls (OAC and Bucket Policies) 11/27/2025 11/27/2025 6 - Deploy Amazon ElastiCache (Redis) cluster + Configure .NET Core application to use Redis for Session Storage integration 11/28/2025 11/28/2025 Week 12 Achievements: Validated project deployment on AWS Cloud. Established CI/CD pipeline using CodePipeline, CodeBuild, and CodeDeploy. Successfully integrated S3 and CloudFront for static asset storage and delivery. Implemented Amazon ElastiCache to handle distributed sessions. Completed introductory modules on AWS Global Infrastructure. "},{"uri":"https://EroEroz.github.io/5-workshop/5.5-app/5.5.3-app-config/","title":"Configure Environment Variables","tags":[],"description":"","content":"To enable the application to connect to Database and Redis, we do not hardcode in the code but use Environment Variables\nGo to Beanstalk Environment \u0026gt; Configuration \u0026gt; Updates, monitoring, and logging \u0026gt; Edit\nScroll down to Environment properties section\nAdd the following variables:\nName: ConnectionStrings__DefaultConnection\nValue: Server=sql-shop-db\u0026hellip;.rds.amazonaws.com;Database=MiniMarketDB;User Id=admin;Password=PASSWORD YOU SET;TrustServerCertificate=True; Name: ConnectionStrings__RedisConnection\nValue: webapp.redis.cache\u0026hellip;:6379 Name: VnPay__IPNUrl\nValue: https://[cloudfrontdomain].cloudfront.net/Payment/VnPayIPN Name: VnPay__ReturnUrl\nValue: https://[cloudfrontdomain].cloudfront.net/Payment/VnPayReturn Click Apply. Server will restart to apply new configuration "},{"uri":"https://EroEroz.github.io/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"“Reinventing DevSecOps” Report Event Purpose Redefine the secure software development process (DevSecOps Lifecycle) from planning to operations. Introduce a comprehensive toolchain to integrate security into every stage of CI/CD. Build a \u0026ldquo;Security-First\u0026rdquo; mindset for development and operations teams. Organizer CMC Global Highlights 1. The DevSecOps Lifecycle The process is divided into 7 closed stages, ensuring security is not a \u0026ldquo;bottleneck\u0026rdquo; but part of the flow:\nPLAN: Identify security requirements \u0026amp; risks right from the start. Align goals between Dev, Sec, and Ops. Create a security roadmap aligned with project goals. CODE: Apply clean code and secure coding standards. Use SAST (Static Application Security Testing) directly in the IDE to detect errors early. Form a \u0026ldquo;Security-first\u0026rdquo; mindset for developers. BUILD: Automate security checks in the CI/CD Pipeline. Perform dependency and binary scans. Ensure secure and consistent builds (Immutable Artifacts). TEST: Run vulnerability scans and DAST (Dynamic Application Security Testing). Perform Penetration Testing. DEPLOY: Check configuration and IaC (Infrastructure as Code) before deployment. Monitor runtime configuration. OPERATE: Automate patching and continuous security updates. Have an incident response process. MONITOR: Continuously monitor for threats. Use Real-time Analytics and alerting tools. 2. DevSecOps Toolchain Overview A robust DevSecOps system requires the coordination of many specialized tools:\nPre-commit \u0026amp; Code Quality: SonarQube, Codacy: Check code quality. GitLeaks: Scan and prevent leaking Secrets/Keys in code before commit. Dependency \u0026amp; SBOM Scanning: Syft, Grype, Dependency-Track: Manage software packages and detect vulnerabilities in 3rd party libraries. IaC \u0026amp; Policy-as-Code: Checkov, Tfsec: Scan for security errors in Terraform/Kubernetes files. OPA Gatekeeper, Kyverno: Enforce compliance policies automatically on Clusters. SAST / DAST \u0026amp; Security Tests: Trivy, Checkmarx: Detect comprehensive vulnerabilities from Code to Runtime. CI/CD Integration: Jenkins, GitHub Actions, GitLab CI, ArgoCD: Platforms to automate the entire process above. Monitoring \u0026amp; Logging: Prometheus, Grafana, Loki: Monitor system health (Observability). Alerting \u0026amp; Governance: Slack, Email, AI Anomaly Detection: Instant alerts when issues occur. What I Learned \u0026ldquo;Shift Left\u0026rdquo; Mindset Security should not be left to the end (Test/Operate stage) but must be shifted left - meaning done right from the Plan and Code stages. Detecting errors early saves repair costs significantly. The Importance of Automation Security cannot be done manually in the Cloud era. Scanning tools need to be integrated into the Pipeline to block faulty builds automatically. Supply Chain Security Risk Management By scanning Dependencies (SBOM), we can prevent attacks on 3rd party libraries (similar to the Log4j incident). Application to Work Integrate GitLeaks: Install pre-commit hook immediately to prevent accidentally pushing AWS Access Keys to GitHub. Deploy SonarQube: Integrate into current CI process to measure technical debt and security vulnerabilities. Apply IaC Scanning: Use Checkov to scan CloudFormation/Terraform files before applying infrastructure to AWS. Monitoring: Set up Grafana Dashboard to monitor real-time application status. Event Experience Although only participating by following the materials, the content from CMC Global provided a very systematic view of DevSecOps.\nClarity in process The slide on DevSecOps Lifecycle helped me visualize the big picture clearly, knowing what needs to be done at each stage instead of just focusing on coding as before. Practical Toolchain The Toolchain slide is a real \u0026ldquo;treasure\u0026rdquo;. It provides a list of Industry Standard tools that I can research and apply immediately to my MiniMarket project (for example, using Trivy to scan Docker Images).\nThe event emphasized that: In the AI and Cloud era, Security is not a feature, but a culture that needs to be built from the very first lines of code.\n"},{"uri":"https://EroEroz.github.io/5-workshop/5.4-data/5.4.3-elasticache/","title":"Initialize ElastiCache Redis","tags":[],"description":"","content":" Access ElastiCache \u0026gt; Subnet groups \u0026gt; Create subnet group Name: redis-private-group Subnets: Select 2 Private Subnets Go to Redis OSS caches \u0026gt; Create cache At Cluster settings screen: Engine: Select Redis OSS Deployment option: Select Node-based cluster Creation method: Select Cluster cache (Configure and create a new cluster) Cluster mode: Select Disabled (Simple mode, 1 Shard) At Location screen:\nLocation: AWS Cloud Multi-AZ: Uncheck (Enable) Note: Disable this feature to save costs for Lab environment Auto-failover: Uncheck (Enable) At Cache settings screen:\nEngine version: Leave default (Ex: 7.1) Port: 6379 Node type: Select t3 family \u0026gt; Select cache.t3.micro Number of replicas: Enter 0 (We only need 1 primary node, no replica node needed) At Connectivity screen: Network type: IPv4 Subnet groups: Select Choose existing subnet group \u0026gt; Select redis-private-group just created At Advanced settings screen (Important): Encryption at rest: Enable (Default) Encryption in transit: Uncheck (Disable) Reason: Disabling encryption in transit simplifies connection from .NET code in internal VPC environment without configuring complex SSL certificates Selected security groups: Select Manage \u0026gt; select sg-redis-cache (Uncheck default) Scroll to the bottom and click Create 3. Get connection information Initialization process will take about 5-10 minutes\nWhen status changes to Available (Green) Click on Cluster name (webapp or name you set) At Overview tab, find Primary endpoint Copy this connection string (Ex: webapp.xxxx.cache.amazonaws.com) This Endpoint will be used to configure ConnectionStrings__RedisConnection environment variable for Elastic Beanstalk in the following steps.\n"},{"uri":"https://EroEroz.github.io/5-workshop/5.3-network/","title":"Network Infrastructure Setup","tags":[],"description":"","content":"Overview In this section, we will build the network foundation for the MiniMarket application. A secure network architecture is a prerequisite to protect the application and data\nWe will design a VPC consisting of:\nPublic Subnet: Dedicated to components communicating directly with the Internet (Load Balancer, NAT Gateway). Private Subnet Dedicated to components requiring security (App Server, Database, Redis) Additionally, we will configure NAT Gateway to allow servers inside the Private Subnet to download updates and Docker Images from the Internet without exposing IP addresses externally\nContent Create VPC \u0026amp; Subnet Configure Internet \u0026amp; NAT Gateway Configure Route Table "},{"uri":"https://EroEroz.github.io/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nThis section will list and introduce the blogs you have translated. For example:\nBlog 1 - Getting started with healthcare data lakes: Using microservices This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 2 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 3 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 4 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 5 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 6 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"},{"uri":"https://EroEroz.github.io/5-workshop/5.4-data/","title":"Data Layer Deployment","tags":[],"description":"","content":"Overview Data is the most important asset of every system. Therefore we will set up the data layer (Data Layer) for MiniMarket with criteria: Maximum Security and High Performance\nWe will deploy two core services:\nAmazon RDS (Relational Database Service): Use SQL Server to store business data (Products, Orders, Users). Database will be placed in Private Subnet to prevent direct access from the Internet Amazon ElastiCache (Redis): Use Redis as cache memory (In-memory Cache) to store Login Sessions and offload queries for the main Database Content Set up Security Groups for DB \u0026amp; Cache Initialize Amazon RDS (SQL Server) Initialize Amazon ElastiCache (Redis) "},{"uri":"https://EroEroz.github.io/4-eventparticipated/4.4-event4/","title":"Event 4","tags":[],"description":"","content":"“AI/ML/GenAI on AWS” Report Event Purpose Provide an overview of Foundation Models and the Amazon Bedrock platform Guide on Prompt Engineering techniques from basic to advanced Explain the architecture and operational workflow of RAG (Retrieval Augmented Generation) Introduce AWS\u0026rsquo;s ecosystem of Pretrained AI services and Frameworks for building AI Agents Speaker List Lam Tuan Kiet Danh Hoanh Hieu Nghi Dinh Le Hoang Anh Highlights Foundation Models \u0026amp; Amazon Bedrock Foundation Models (FMs): Trained using self-supervised training, capable of processing many tasks. Amazon Bedrock: Platform providing access to leading models like Luma, Deepseek\u0026hellip; Prompt Engineering (AI Command Techniques) Basic process: Prompt → Bedrock → Response. Key techniques:\nZero-Shot Prompting: Asking questions directly without providing sample data. Few-Shot Prompting: Providing a few examples (example frame) for the model to learn the answer structure when encountering similar questions. Chain of Thought (CoT): Guiding AI on step-by-step reasoning, helping AI provide more accurate answers based on learned logic. Retrieval Augmented Generation (RAG) Currently popular model (used widely in Banking). Process includes: Retrieval → Augmentation → Generation.\nEmbeddings: Convert human language into Vectors. Using Amazon Titan Embedding (supports multiple languages). RAG in Action: Data preparation: Data source → Document chunk → Embeddings model → Vector store. Query processing: User input → Embeddings model → Vector → Semantic search (in Vector store) → Context → Prompt augmentation → LLM → Response. Other Pretrained AI Services AWS provides specialized services with optimal costs:\nAmazon Rekognition (Computer Vision): Image/video analysis, face and object detection. Pricing: ~$0.0013/image. Amazon Translate: Real-time or batch text translation. Pricing: ~$15/million characters. Amazon Textract: Extract text and layout from documents (OCR). Pricing: ~$0.05/page. Amazon Transcribe: Convert speech to text, supports streaming. Amazon Polly: Convert text to speech, supports Real-time TTS. Pricing: ~$4/million characters. Amazon Comprehend (NLP): Sentiment Analysis, key phrase extraction, PII detection. Pricing: ~$0.0001/100 chars. Amazon Kendra: Intelligent Search service, supports Natural Language Search and RAG. Amazon Lookout Family: Detect anomalies in Metrics, Equipment, and Vision. Amazon Personalize: Personalized recommendation system for users. AI Agents \u0026amp; Frameworks Pipecat: Framework for voice/multimodal AI agents, optimized for real-time conversation assistants. Amazon Bedrock AgentCore: Supported frameworks: Langgraph, Langchain, Strands Agents SDK. Process from Idea → Production needs to focus on: Performance, Scalability, Security, Governance. Core components: Runtime, Memory, Identity, Gateway, Code Interpreter, Browser tool, Observability. What I Learned Prompting Mindset Optimization Techniques: Clearly understand the differences between Zero-shot, Few-shot, and Chain of Thought to apply to specific problem complexities. Structuring: Providing examples helps better control the model\u0026rsquo;s output format. RAG Architecture Vector Database: Understand the critical role of Vector Store and Semantic Search in providing accurate context for LLMs. Embedding Models: The importance of choosing embedding models (like Amazon Titan) to support multiple languages and search accuracy. Service Selection (Trade-offs) Cost vs Flexibility: Know how to balance costs between using Pretrained Services (pay per request/char) versus building custom models or using general LLMs. Use case specific: Each service (Rekognition, Textract\u0026hellip;) solves a specific problem better and cheaper than forcing an LLM to do everything. Application to Work Deploy RAG: Apply Retrieval Augmented Generation model to build internal search systems for enterprise/banking. Integrate Pretrained Services: Use Amazon Textract and Comprehend to automate document and record processing. Build intelligent Chatbots: Combine Amazon Lex/Bedrock with Pipecat framework to create real-time conversational virtual assistants. Cost Optimization: Use Caching for Amazon Polly or select the right tier of AI services to minimize operating costs. Event Experience Participating in the “Generative AI with Amazon Bedrock” event helped me systematize knowledge about GenAI and the AWS ecosystem. Some highlights:\nPractical Knowledge from Experts Speakers shared real-world experiences on applying RAG and Prompt Engineering. Deeper understanding of the AI reasoning process through Chain of Thought. Overview of AI Services Not stopping at LLMs, the event provided a broad view of Specialized AI Services like Lookout, Kendra, Personalize\u0026hellip; helping solve niche problems effectively. The Pricing analysis gave me more data to make decisions when designing solutions. Tech Trend Updates Introduced to Agentic AI and components of Bedrock AgentCore, opening new directions in building AI applications capable of executing complex tasks (Idea to Production).\nApproached Pipecat framework, an optimal solution for voice AI agents.\nIn conclusion, the event provided a solid knowledge foundation about Amazon Bedrock and GenAI techniques, thereby helping me feel more confident in proposing and implementing AI solutions for projects.\n"},{"uri":"https://EroEroz.github.io/5-workshop/5.5-app/","title":"Application Deployment","tags":[],"description":"","content":"Overview After having network and data infrastructure, the next step is to bring .NET Core application source code to Cloud. Instead of managing each EC2 virtual server manually, we will use the Platform-as-a-Service (PaaS) platform which is AWS Elastic Beanstalk\nGoals of this module:\nContainerization: Package MiniMarket application into Docker Container to ensure uniform running environment (Dev = Prod) Deployment: Deploy Container to Elastic Beanstalk. The system will automatically provision EC2, configure Load Balancer and Auto Scaling Group Connectivity: Configure so application connects securely to RDS and Redis via Environment Variables Content Package application with Docker Initialize Elastic Beanstalk Environment Configure Database \u0026amp; Redis connection "},{"uri":"https://EroEroz.github.io/4-eventparticipated/4.5-event5/","title":"Event 5","tags":[],"description":"","content":"“DevOps on AWS” Report Event Purpose Guide career paths in DevOps and Cloud fields. Deeply understand CI/CD processes and Containerization. Analyze the role of Infrastructure as Code (IaC) versus ClickOps. Compare Orchestration solutions on AWS (ECS vs EKS) and Monitoring/Observability strategies. Highlights Next-Generation DevOps Roadmap Related roles: DevOps Engineer, Cloud Engineer, Platform Engineer, Site Reliability Engineer (SRE). T-shaped Skill: A skill development model with broad knowledge across many areas and deep knowledge in one specific area. Advice for beginners: Do: Start with Fundamentals, learn through Real Projects, Document everything, focus on mastering one skill at a time. Don\u0026rsquo;t: Get stuck in \u0026ldquo;Tutorial Hell\u0026rdquo;, copy-paste blindly, compare yourself to others, give up after failures. Continuous Integration \u0026amp; Deployment (CI/CD) CI (Continuous Integration): The process of frequently integrating code into a shared repository. Distinguishing CD: Continuous Delivery: Automated up to Acceptance Test; deployment to Production requires a manual trigger. Continuous Deployment: Fully automated from code to Production. Infrastructure as Code (IaC) Problems with ClickOps: Slow, prone to Human Error, inconsistent, difficult to collaborate. Benefits of IaC: Automation, Scalability, Reproducibility, enhanced collaboration. AWS CloudFormation: AWS\u0026rsquo;s built-in IaC tool using JSON/YAML templates. Stack: A collection of resources managed together. Drift Detection: Detects discrepancies between actual configuration and the template (when someone manually modifies resources). AWS CDK (Cloud Development Kit): Uses programming languages (Python, TS\u0026hellip;) to define infrastructure. Concepts: L1 (Mapping 1:1), L2, L3 Constructs. CLI commands: cdk init, cdk synth, cdk deploy, cdk diff, cdk destroy. Terraform: Open-source tool, supports Multi-cloud, uses HCL language, manages state via State file. Container Ecosystem Docker Fundamentals: Dockerfile (build definition) → Image (packaging blueprint) → Container (runtime). Amazon ECR: AWS\u0026rsquo;s private container registry, supporting image scanning, immutable tags and lifecycle policies. Container Orchestration: Manages container lifecycle (restart, scale, distribute traffic). Amazon ECS: AWS native solution. Supports Launch types: EC2 (server management) and Fargate (Serverless - easier). Amazon EKS: Managed Kubernetes service. Suitable for complex systems requiring K8s experience. Amazon App Runner: Simplest solution to deploy web apps/APIs quickly and cost-effectively. Monitoring \u0026amp; Observability Distinction: Monitoring: Tracking Logs, Metrics (How is the system performing?). Observability: Deeply understanding root causes (Why is the system performing this way?). Amazon CloudWatch: Collects Metrics (CPU, RAM, Network\u0026hellip;) and Logs in real-time. Alarms: Alerts and automated responses. Dashboards: Visualize operational data. AWS X-Ray: Distributed tracing for microservices, helping visualize service maps and analyze performance bottlenecks. What I Learned System Thinking Infrastructure Drift: Understanding the risks of manual resource modification outside of IaC and the importance of using Drift Detection. Trade-offs: Knowing how to select IaC tools (CDK for AWS-centric, Terraform for Multi-cloud) and Orchestration tools (ECS for beginners/simple needs, EKS for advanced features). Operational Skills Standard Process: The core difference between Continuous Delivery and Continuous Deployment lies in the manual approval step to Production. Container Management: Understanding the workflow from Dockerfile to ECR and how ECS/EKS orchestrates container operations. Application to Work Transition to IaC: Start writing CloudFormation or CDK for current projects instead of operating on the Console. Optimize Pipeline: Review CI/CD processes, integrate automated testing steps before deployment. Implement Observability: Integrate AWS X-Ray into applications to trace requests across microservices, combined with CloudWatch Alarms for proactive monitoring. Refactor Docker: Optimize Dockerfiles and use ECR Lifecycle Policies to manage image storage space. Event Experience Participating in the “Next-Generation DevOps \u0026amp; Cloud Architecture” event was a crucial stepping stone helping me clearly define my DevOps skill development path.\nClear Orientation The speaker outlined a very practical learning Roadmap with the advice \u0026ldquo;Don\u0026rsquo;t stay in Tutorial Hell\u0026rdquo; - which resonated with me. The T-shaped skill model helped me realize I don\u0026rsquo;t need to know everything at once but should focus deeply on one area first. In-depth Tool Knowledge The detailed comparison between ECS and EKS made me more confident in choosing the right compute solution for my project (as a beginner, ECS Fargate is the optimal choice). The presentation on IaC and Drift Detection completely changed my mindset on infrastructure management: \u0026ldquo;Use code, not clicks\u0026rdquo;. The Importance of Observability I realized that Monitoring (looking at charts) is not enough; achieving Observability (understanding the nature) through tools like AWS X-Ray is necessary to thoroughly resolve issues in distributed systems. Some photos from the event Add your photos here This event not only provided technical knowledge but also inspired a professional mindset, from building CI/CD culture to automating everything possible.\n"},{"uri":"https://EroEroz.github.io/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Deploying Cloud-Native MiniMarket Application on AWS Overview This workshop provides a comprehensive guide on re-platforming the MiniMarket e-commerce application (developed on the .NET Core platform) from a local environment to the AWS cloud infrastructure using a Cloud Native architecture.\nWe will not merely rent a virtual server (EC2) to run code. Instead, we will build a distributed system that is highly scalable, secure, and automated based on managed services.\nWe will establish a Multi-tier architecture consisting of core components:\nCompute: Use AWS Elastic Beanstalk combined with Docker to simplify application deployment and management, supporting automatic Auto Scaling based on traffic. Data \u0026amp; Caching: Migrate from local SQL Server to Amazon RDS (placed in Private Subnet) to ensure data security. Simultaneously, deploy Amazon ElastiCache (Redis) to manage User Sessions, ensuring high performance for the application. Networking \u0026amp; Security: Use VPC along with Public/Private Subnet and NAT Gateway for secure outbound connection, and protect the application against attacks using AWS WAF combined with CloudFront. DevOps: Build a CI/CD process using AWS CodePipeline and CodeBuild, allowing automation of the process from committing code to GitHub until the application runs in the Production environment Content Workshop Overview Prerequisites Network Infrastructure Setup (VPC, NAT, Security Groups) Data Layer Deployment (RDS \u0026amp; Redis) Application Deployment with Elastic Beanstalk \u0026amp; Docker Automation with CI/CD Pipeline Optimization and Security(S3, CloudFront, WAF) Monitoring (CloudWatch) Resource Cleanup "},{"uri":"https://EroEroz.github.io/5-workshop/5.6-cicd/","title":"CI/CD Automation","tags":[],"description":"","content":"Overview In the Cloud Native environment, manual deployment (Manual Deployment) is risky and time-consuming. This module will guide you to build a fully automated CI/CD (Continuous Integration / Continuous Deployment) process\nThe process operates as follows:\nSource: Developer pushes code (Push) to GitHub Build: AWS CodePipeline detects changes and activates AWS CodeBuild. CodeBuild will package the Docker Image and push to the Amazon ECR repository Deploy: Pipeline automatically commands Elastic Beanstalk to update the latest version from ECR without service interruption Content Create Build Project with AWS CodeBuild Set up AWS CodePipeline "},{"uri":"https://EroEroz.github.io/4-eventparticipated/4.6-event6/","title":"Event 6","tags":[],"description":"","content":"“AWS Security Governance \u0026amp; Automation” Report Event Purpose Introduce the vision of Cloud Club and the importance of community. Share methods for centralized Identity Management and account security. Guide on Multi-Layer Security Visibility strategy. Automate security incident response process (Automated Alerting) with EventBridge. Highlights Identity \u0026amp; Access Management (IAM \u0026amp; Governance) Single Sign-On (SSO): \u0026ldquo;One login, multiple systems\u0026rdquo; mechanism. Instead of creating many scattered IAM Users, SSO allows centralized identity management, helping users log in once to access multiple AWS accounts/applications. Service Control Policies (SCPs): A type of Organizational Policy used to set up \u0026ldquo;guardrails\u0026rdquo;. SCPs limit maximum permissions for member accounts in AWS Organization. Credentials Spectrum: Long-term: IAM User Access Keys (do not expire) → High risk, need to limit usage. Short-term: IAM Roles, STS tokens (expire after 15 minutes - 36 hours) → Higher security, is best practice. MFA (Multi-Factor Authentication): Mandatory security layer for every account. Multi-Layer Security Visibility IAM Access Analyzer: Tool helping detect resources (S3, KMS, IAM Roles\u0026hellip;) being publicly shared or shared with untrusted accounts. Event Classification (Logging): Management Events: Who did what to resources? (Ex: Create EC2, Delete S3 Bucket). Data Events: Who accessed the data? (Ex: GetObject S3, Invoke Lambda). Network Activity Events: Monitor VPC network traffic. Automation \u0026amp; Alerting Amazon EventBridge: Real-time Event processing center. Supports Cross-account Event: Receive events from child accounts to the central security account. Automated Alerting: Automatically send alerts when abnormal behavior is detected. Detection as Code: Convert threat detection logic into code (Infrastructure as Code). Use CloudTrail Lake queries to query history. Version control for security rules. Network Security Common Attack Vectors: Common network attack vectors (DDoS, SQL Injection, Man-in-the-middle\u0026hellip;). AWS Layered Security: Defense in Depth strategy - protecting from the outer layer (Edge), network layer (VPC), to application and data layers. What I Learned Modern Security Mindset Identity is the new perimeter: In Cloud environment, identity management (IAM/SSO) is more important than traditional firewalls. Zero Trust: Trust no one, always authenticate (MFA) and grant least privilege. Governance Strategy Shift from Long-term to Short-term: Understand clearly the risks of long-term Access Keys and the importance of shifting to Temporary Credentials. Governance at Scale: Use SCPs to manage hundreds of AWS accounts consistently instead of manual configuration for each. Automation Techniques Event-Driven Security: Instead of manual log review (passive), use EventBridge to react immediately (active) when security incidents occur. Application to Work Review IAM: Check and disable old IAM Access Keys, enforce MFA for the whole team. Deploy Access Analyzer: Activate immediately to scan if any S3 bucket is mistakenly public. Set up alerts: Create simple EventBridge rules to send notifications to Slack/Email when someone logs in with Root account or changes Security Group. Learn about CloudTrail: Configure CloudTrail to record both Management and Data events for critical resources. Event Experience The 3rd event brought a very deep perspective on Security \u0026amp; Governance aspects - an area often overlooked in development but vital for businesses.\nRisk Awareness The presentation on Credentials Spectrum startled me into realizing how dangerous the habit of using IAM Users with Long-term keys is. Shifting to Short-term credentials is mandatory. The Power of Automation I was very impressed with the concept of \u0026ldquo;Detection as Code\u0026rdquo;. Managing security rules like source code helps operations become transparent and easier to control. The EventBridge demo showed excellent real-time response capabilities, minimizing the time attackers can cause harm in the system. Multi-layered Defense Mindset Understood better about AWS Layered Security, security is not just a door but multiple barrier layers coordinating from Network to Identity.\nThis event changed my mindset from \u0026ldquo;Make it run\u0026rdquo; to \u0026ldquo;Make it safe and compliant\u0026rdquo;. Security is not a barrier, but a foundation for sustainable development.\n"},{"uri":"https://EroEroz.github.io/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"My internship at Amazon Web Services Vietnam Co., Ltd. from August 12, 2025 to November 12, 2025 was a transformative period where I applied theoretical knowledge to build scalable, production-grade cloud systems.\nMy primary responsibility involved designing the Solution Architecture based on the AWS Well-Architected Framework and setting up Automated Operations (CI/CD). Through this process, I have rapidly matured my skills in Cloud Engineering, critical analysis, and technical reporting within enterprise environment.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ✅ ☐ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ✅ ☐ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Deepen knowledge of Cloud troubleshooting \u0026amp; system optimization. Improve the ability to explain architectures to others people "},{"uri":"https://EroEroz.github.io/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"During the internship, I participated in 6 events. Each event was a memorable experience providing new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: AWS Cloud Day 2025\nTime: 9:00 on Septemper 18, 2025\nLocation: 26th Floor, Bitexco Financial Tower, No. 02 Hai Trieu Street, Sai Gon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: Data Science on AWS Workshop\nTime: 9:30 on October 16, 2025\nLocation: Hall Academic – FPT University\nRole: Attendee\nEvent 3 Event Name: Reinventing DevSecOps with AWS Generative AI\nTime: 19:30 on October 16, 2025\nLocation: Online via Microsoft Teams\nRole: Attendee\nEvent 4 Event Name: AI/ML/GenAI on AWS\nTime: 8:00 on November 15, 2025\nLocation: 26th Floor, Bitexco Financial Tower, No. 02 Hai Trieu Street, Sai Gon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 5 Event Name: DevOps on AWS\nTime: 8:30 on November 17, 2025\nLocation: 26th Floor, Bitexco Financial Tower, No. 02 Hai Trieu Street, Sai Gon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 6 Event Name: AWS Well-Architected Security Pillar\nTime: 8:30 on November 29, 2025\nLocation: 26th Floor, Bitexco Financial Tower, No. 02 Hai Trieu Street, Sai Gon Ward, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://EroEroz.github.io/5-workshop/5.7-security/","title":"Optimization &amp; Security","tags":[],"description":"","content":"Resource Cleanup Overview A Production system needs not only to \u0026ldquo;run\u0026rdquo; but also to \u0026ldquo;run fast\u0026rdquo; and be \u0026ldquo;secure\u0026rdquo;. In this part, we will refine the MiniMarket architecture\nImplementation items:\nOffloading Static Assets: Transfer all product images from Web Server to Amazon S3 and distribute via Amazon CloudFront (CDN) to accelerate global page load speed and offload the server Security Hardening: Deploy AWS WAF (Web Application Firewall) in front of CloudFront to protect the application from common attacks such as SQL Injection and XSS Content Configure S3 and CloudFront Set up AWS WAF "},{"uri":"https://EroEroz.github.io/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"Overall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always ready to support me whenever I face difficulties, even outside of working hours. The workspace is tidy and comfortable, which helps me focus better.\n2. Support from Mentor / Team Admin\nThe Team Admin supported me by guiding problem-solving mindsets and creating favorable conditions for me to work effectively.\n3. Relevance of Work to Academic Major\nThe assigned tasks align well with my academic background while allowing me to access modern Cloud technologies that meet enterprise needs. As a result, I have acquired many practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork, and professional communication in a corporate environment. Mentors also shared practical experiences that helped me better orient my career path.\n5. Company Culture \u0026amp; Team Spirit\nThere is mutual respect among everyone. The team maintains a professional attitude when necessary but also knows how to have fun to keep the atmosphere stress-free.\n6. Internship Policies / Benefits\nThe company facilitates interns participation in events to gain new knowledge for project completion, as well as for future application. Additional Questions What did you find most satisfying during your internship? Being facilitated to attend workshops and events to broaden my knowledge. What do you think the company should improve for future interns? Increase direct working time to make technical exchange more effective. If recommending to a friend, would you suggest they intern here? Why or why not? Yes. If my friends want to gain substantial new knowledge, I would suggest they intern here. Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Organize more workshops for further learning. Would you like to continue this program in the future? Yes. Any other comments (free sharing): "},{"uri":"https://EroEroz.github.io/5-workshop/5.8-monitoring/","title":"Monitoring &amp; Operations","tags":[],"description":"","content":"Overview A Production system cannot be considered complete without Monitoring and Alerting capabilities. You cannot sit watching the screen 24/7 to check if the Server is still alive\nIn this module, we will set up monitoring and alerting for MiniMarket using AWS operations management services:\nAmazon CloudWatch: Collect metrics (Metrics) from EC2, RDS, ELB Amazon SNS (Simple Notification Service): Notification service. We will use it to send Emails to administrators when the system encounters issues We will set up a CloudWatch Alarm to monitor Web Server CPU. If CPU exceeds 70% (sign of overload or attack), the system will automatically trigger SNS to send emergency alert Emails\nContent Set up CloudWatch Alarms \u0026amp; SNS "},{"uri":"https://EroEroz.github.io/5-workshop/5.9-cleanup/","title":"Resource Cleanup","tags":[],"description":"","content":"Overview Congratulations on successfully deploying MiniMarket on AWS!\nHowever, our work does not end there. The final step and also the most important step to protect your \u0026ldquo;wallet\u0026rdquo; is Resource Cleanup\nThe services we deployed such as NAT Gateway, Elastic Load Balancer, RDS, ElastiCache are all billed by the hour, whether you use them or not. If you forget to delete, the month-end bill can be very high\nWe will go through the system Decommissioning process in the correct order to ensure no resources are left behind causing hidden costs\nContent Safe resource deletion process "},{"uri":"https://EroEroz.github.io/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://EroEroz.github.io/tags/","title":"Tags","tags":[],"description":"","content":""}]